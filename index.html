<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
      LiteAttention: A Temporal Sparse Attention for Diffusion Transformers
    </title>
    <meta
      name="description"
      content="LiteAttention achieves up to 54% attention sparsity on production video diffusion models with no degradation in generation quality."
    />
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto",
          "Oxygen", "Ubuntu", "Cantarell", sans-serif;
        line-height: 1.6;
        color: #333;
        background: #ffffff;
      }

      .container {
        max-width: 1100px;
        margin: 0 auto;
        padding: 40px 40px;
      }

      header {
        text-align: center;
        padding: 60px 20px 40px;
        margin-bottom: 60px;
        background: #d2dcef;
        /* Make header background full-bleed across viewport */
        box-shadow: 0 0 0 100vmax #d2dcef;
        clip-path: inset(0 -100vmax 0 -100vmax);
      }

      .site-logo {
        display: block;
        margin: 0 auto 20px;
        max-width: 330px;
        height: auto;
      }

      h1 {
        font-size: 1.8rem;
        margin-bottom: 30px;
        color: #000;
        font-weight: 700;
        line-height: 1.2;
      }

      .authors {
        margin: 25px 0;
        font-size: 1.15em;
        color: #333;
        line-height: 1.8;
      }

      .authors a {
        color: #0066cc;
        text-decoration: none;
      }

      .authors a:hover {
        text-decoration: underline;
      }

      .affiliation {
        margin-top: 15px;
        font-size: 1.05em;
        color: #666;
      }

      .affiliation a {
        color: #0066cc;
        text-decoration: none;
      }

      .affiliation a:hover {
        text-decoration: underline;
      }

      .links {
        display: flex;
        gap: 15px;
        justify-content: center;
        flex-wrap: wrap;
        margin-top: 10px;
      }

      .btn {
        display: inline-block;
        padding: 8px 22px;
        background: #1b4fa4;
        color: white;
        text-decoration: none;
        border-radius: 9999px;
        font-weight: 500;
        transition: background 0.2s;
        font-size: 0.9em;
      }

      .btn:hover {
        background: #17458b;
      }

      /* Video bricks: rows of 2 / 3 / 2 / 3 / 2 (16:9 tiles) */
      .section-overview {
        margin-top: 60px;
        background: #d2dcef;
        padding: 30px 0;
        box-shadow: 0 0 0 100vmax #d2dcef;
        clip-path: inset(0 -100vmax 0 -100vmax);
      }

      /* Full-bleed colored section like header */
      .section-algo,
      .section-blue {
        background: #d2dcef;
        padding: 30px 0;
        box-shadow: 0 0 0 100vmax #d2dcef;
        clip-path: inset(0 -100vmax 0 -100vmax);
      }

      /* Full-bleed gallery inside .section-blue */
      .section-blue .video-grid {
        display: flex;
        flex-wrap: nowrap;
        justify-content: center;
        align-items: flex-start;
        gap: 24px;
        overflow-x: auto;

        width: 100vw; /* span the viewport */
        margin-left: calc(50% - 50vw); /* break out of centered container */
        padding: 0 40px; /* edge padding */
        box-sizing: border-box;
        scroll-snap-type: x proximity; /* nicer horizontal scroll (optional) */
      }

      .section-blue .video-item {
        flex: 0 0 auto;
        text-align: center;
        scroll-snap-align: start; /* optional */
      }

      .section-blue .video-item img {
        display: block;
        height: auto;
        max-height: 360px; /* keep consistent height */
        width: auto; /* avoid stretching/narrow crops */
        border-radius: 10px;
        border: 1px solid #e0e0e0;
      }

      /* Quick Start full-bleed */
      .section-quickstart {
        background: #d2dcef;
        padding: 30px 0;
        box-shadow: 0 0 0 100vmax #d2dcef;
        clip-path: inset(0 -100vmax 0 -100vmax);
      }
      .video-bricks {
        display: grid;
        gap: 8px;
        width: 100%;
        max-width: 1100px;
        margin: 20px auto 10px;
        padding: 0 10px;
      }

      .brick-row {
        display: grid;
        gap: 12px;
      }

      .brick-row.cols-2 {
        grid-template-columns: repeat(2, 1fr);
      }
      .brick-row.cols-3 {
        grid-template-columns: repeat(3, 1fr);
      }
      .brick-row.cols-4 {
        grid-template-columns: repeat(4, 1fr);
      }
      .brick-row.cols-5 {
        grid-template-columns: repeat(5, 1fr);
      }

      .video-tile {
        position: relative;
        width: 100%;
        aspect-ratio: 16 / 9;
        background: #f4f7fb;
        border: 1px solid #e0e0e0;
        border-radius: 8px;
        overflow: hidden;
      }

      .spacer {
        aspect-ratio: 16 / 9;
      }

      .video-tile video,
      .video-tile img {
        width: 100%;
        height: 100%;
        object-fit: cover;
        display: block;
      }

      /* LA mask removed; using simple brick layout */

      section {
        margin-bottom: 70px;
      }

      h2 {
        font-size: 1.5rem;
        margin-bottom: 30px;
        color: #000;
        font-weight: 700;
        padding-bottom: 15px;
        text-align: center;
      }

      h3 {
        font-size: 1.25rem;
        margin: 35px 0 20px;
        color: #000;
        font-weight: 600;
        max-width: 760px;
        margin-left: auto;
        margin-right: auto;
      }

      p {
        font-size: 1.05em;
        line-height: 1.8;
        color: #333;
        margin-bottom: 20px;
        max-width: 760px;
        margin-left: auto;
        margin-right: auto;
      }

      .tldr {
        background: #f8f9fa;
        padding: 30px;
        border-left: 4px solid #0066cc;
        margin: 30px 0;
        border-radius: 4px;
      }

      .tldr p {
        margin: 0;
        font-size: 1.15em;
      }

      .features {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(240px, 1fr));
        gap: 30px;
        margin: 40px 0;
      }

      .feature {
        padding: 0;
      }

      .feature h4 {
        font-size: 1.15em;
        margin-bottom: 12px;
        color: #000;
        font-weight: 600;
      }

      .feature p {
        color: #555;
        line-height: 1.7;
        font-size: 1em;
      }

      /* New feature cards */
      .feature-cards {
        display: grid;
        grid-template-columns: repeat(2, 1fr);
        gap: 24px;
        max-width: 760px;
        margin: 10px auto 0;
      }

      .feature-card {
        background: linear-gradient(135deg, #d2dcee 0%, #678ac3 100%);
        border-radius: 20px;
        padding: 26px 28px;
        color: #0b1b3b;
        box-shadow: 0 6px 16px rgba(0, 0, 0, 0.08);
        transition: transform 0.25s ease, box-shadow 0.25s ease;
      }

      .feature-card:hover {
        transform: translateY(-6px);
        box-shadow: 0 14px 28px rgba(0, 0, 0, 0.12);
      }

      .feature-card h4 {
        margin: 0 0 12px;
        font-weight: 700;
        color: #0b1b3b;
        text-align: center;
      }

      .feature-card p {
        margin: 0;
        color: #0b1b3b;
        text-align: center;
      }

      table {
        width: 100%;
        max-width: 760px;
        border-collapse: collapse;
        margin: 30px auto;
        font-size: 0.95em;
      }

      th,
      td {
        padding: 12px 15px;
        text-align: left;
        border-bottom: 1px solid #e0e0e0;
      }

      th {
        background: #f8f9fa;
        color: #333;
        font-weight: 600;
        border-top: 2px solid #e0e0e0;
      }

      tbody tr:hover {
        background: #f8f9fa;
      }

      .video-grid {
        display: flex;
        flex-wrap: nowrap;
        justify-content: center;
        align-items: flex-start;
        gap: 20px;
        overflow-x: auto;
        padding: 10px 0;
      }

      /* 4-way video comparison */
      .video-comp-grid {
        display: grid;
        grid-template-columns: repeat(2, 1fr);
        column-gap: 12px;
        row-gap: 28px;
        max-width: 1100px;
        margin: 30px auto 20px;
        padding: 0 10px;
      }

      .video-comp-item {
        display: flex;
        flex-direction: column;
        gap: 8px;
      }

      .video-comp-title {
        font-size: 0.95rem;
        font-weight: 600;
        text-align: center;
        color: #0b1b3b;
      }

      .video-comp-grid video {
        width: 100%;
        aspect-ratio: 16 / 9;
        border-radius: 8px;
        border: 1px solid #e0e0e0;
        display: block;
        object-fit: cover;
      }

      .video-comp-grid video.highlight-lite {
        border: 3px solid #1b4fa4;
      }

      .video-comp-title-lite {
        color: #1b4fa4;
        font-size: 1.1rem;
        font-weight: 700;
      }

      .video-comp-title-muted {
        color: #646464;
      }

      .video-comp-title-strong {
        font-size: 1.1rem;
        font-weight: 700;
      }
      .video-item {
        flex: 0 0 auto;
        text-align: center;
      }

      .video-item img {
        width: 100%;
        border-radius: 4px;
        margin-bottom: 15px;
        border: 1px solid #e0e0e0;
      }

      .video-caption {
        font-weight: 600;
        color: #333;
        margin-bottom: 5px;
        font-size: 1em;
      }

      .video-time {
        color: #0066cc;
        font-weight: 600;
        font-size: 1.1em;
      }

      code {
        background: #f4f4f4;
        color: #333;
        padding: 2px 6px;
        border-radius: 3px;
        font-family: "Monaco", "Menlo", "Courier New", monospace;
        font-size: 0.9em;
      }

      pre {
        background: #f8f9fa;
        border: 1px solid #e0e0e0;
        padding: 20px;
        border-radius: 4px;
        overflow-x: auto;
        margin: 25px auto;
        max-width: 760px;
      }

      pre code {
        background: none;
        padding: 0;
        color: #333;
      }

      .highlight {
        font-weight: 600;
        color: #000;
      }

      ul {
        margin: 20px 0;
        padding-left: 0;
        list-style: disc inside;
        max-width: 760px;
        margin-left: auto;
        margin-right: auto;
      }

      ul li {
        padding: 6px 0;
        position: static;
        line-height: 1.7;
        max-width: 760px;
        margin-left: auto;
        margin-right: auto;
      }

      ul li:before {
        content: none;
      }

      .algo-box {
        background: #ffffff;
        padding: 26px 28px;
        border-radius: 20px;
        margin: 30px 0;
        max-width: 760px;
        margin-left: auto;
        margin-right: auto;
        color: #0b1b3b;
        box-shadow: 0 6px 16px rgba(0, 0, 0, 0.08);
        transition: transform 0.25s ease, box-shadow 0.25s ease;
      }

      .algo-box:hover {
        transform: translateY(-6px);
        box-shadow: 0 14px 28px rgba(0, 0, 0, 0.12);
      }

      .algo-box h4 {
        margin: 0 0 12px;
        color: #0b1b3b;
        font-size: 1.1em;
        font-weight: 700;
        text-align: left;
      }

      .algo-box p {
        margin: 0 0 12px;
        color: #0b1b3b;
        text-align: left;
      }

      footer {
        text-align: center;
        padding: 20px 20px 30px;
        color: #666;
        font-size: 0.95em;
        border-top: 1px solid #e0e0e0;
      }

      footer a {
        color: #0066cc;
        text-decoration: none;
      }

      footer a:hover {
        text-decoration: underline;
      }

      @media (max-width: 768px) {
        h1 {
          font-size: 1.8rem;
        }

        h2 {
          font-size: 1.5rem;
        }

        .features {
          grid-template-columns: 1fr;
        }

        .feature-cards {
          grid-template-columns: 1fr;
        }

        .video-comp-grid {
          grid-template-columns: 1fr;
        }

        .brick-row.cols-2,
        .brick-row.cols-3 {
          grid-template-columns: 1fr;
        }
        .brick-row.cols-4 {
          grid-template-columns: 1fr;
        }
        .brick-row.cols-5 {
          grid-template-columns: 1fr;
        }
        .spacer {
          display: none;
        }
      }
    </style>
  </head>
  <body>
    <div class="container">
      <header>
        <img class="site-logo" src="assets/images/logo.png" alt="Logo" />
        <h1>
          LiteAttention: A Temporal Sparse Attention for Diffusion Transformers
        </h1>
        <div class="authors">
          Dor Shmilovich, Tony Wu, Aviad Dahan, Yuval Domb
        </div>
        <div class="links">
          <a href="https://arxiv.org/pdf/2511.11062" class="btn">Paper</a>
          <a href="https://github.com/moonmath-ai/LiteAttention" class="btn"
            >Code</a
          >
          <a href="https://arxiv.org/abs/2511.11062" class="btn">arXiv</a>
          <a href="https://moonmath.ai" class="btn">MoonMath.ai</a>
          <a href="mailto:research@moonmath.ai" class="btn">Contact Us</a>
        </div>
      </header>

      <div class="video-bricks">
        <div class="brick-row cols-4">
          <div class="video-tile" data-video="1">
            <video
              src="assets/VideoGrid/t2v-A14B_1280*720_1_A_bustling_city_street_at_night,_filled_with_the_g_20251107_124502.mp4"
              autoplay
              muted
              loop
              playsinline
              preload="metadata"
            ></video>
          </div>
          <div class="video-tile" data-video="2">
            <video
              src="assets/VideoGrid/t2v-A14B_1280*720_1_A_detailed_wooden_toy_ship_with_intricately_carved_20251107_124528.mp4"
              autoplay
              muted
              loop
              playsinline
              preload="metadata"
            ></video>
          </div>
          <div class="video-tile" data-video="3">
            <video
              src="assets/VideoGrid/t2v-A14B_1280*720_1_A_majestic_beauty_of_a_waterfall_cascading_down_a__20251107_122850.mp4"
              autoplay
              muted
              loop
              playsinline
              preload="metadata"
            ></video>
          </div>
          <div class="video-tile" data-video="4">
            <video
              src="assets/VideoGrid/t2v-A14B_1280*720_1_A_serene_night_scene_in_a_forested_area._The_first_20251107_124537.mp4"
              autoplay
              muted
              loop
              playsinline
              preload="metadata"
            ></video>
          </div>
        </div>
        <div class="brick-row cols-4">
          <div class="video-tile" data-video="5">
            <video
              src="assets/VideoGrid/t2v-A14B_1280*720_1_A_serene_underwater_scene_featuring_a_sea_turtle_s_20251107_122832.mp4"
              autoplay
              muted
              loop
              playsinline
              preload="metadata"
            ></video>
          </div>
          <div class="video-tile" data-video="6">
            <video
              src="assets/VideoGrid/t2v-A14B_1280*720_1_A_snowy_forest_landscape_with_a_dirt_road_running__20251107_124514.mp4"
              autoplay
              muted
              loop
              playsinline
              preload="metadata"
            ></video>
          </div>
          <div class="video-tile" data-video="7">
            <video
              src="assets/VideoGrid/t2v-A14B_1280*720_1_A_soaring_drone_footage_captures_the_majestic_beau_20251107_122844.mp4"
              autoplay
              muted
              loop
              playsinline
              preload="metadata"
            ></video>
          </div>
          <div class="video-tile" data-video="8">
            <video
              src="assets/VideoGrid/t2v-A14B_1280*720_1_A_vibrant_scene_of_a_snowy_mountain_landscape._The_20251107_122851.mp4"
              autoplay
              muted
              loop
              playsinline
              preload="metadata"
            ></video>
          </div>
        </div>
        <div class="brick-row cols-4">
          <div class="video-tile" data-video="9">
            <video
              src="assets/VideoGrid/t2v-A14B_1280*720_1_A_vibrant_underwater_scene._A_group_of_blue_fish,__20251107_122859.mp4"
              autoplay
              muted
              loop
              playsinline
              preload="metadata"
            ></video>
          </div>
          <div class="video-tile" data-video="10">
            <video
              src="assets/VideoGrid/t2v-A14B_1280*720_1_The_camera_follows_behind_a_white_vintage_SUV_with_20251107_124548.mp4"
              autoplay
              muted
              loop
              playsinline
              preload="metadata"
            ></video>
          </div>
          <div class="video-tile" data-video="11">
            <video
              src="assets/VideoGrid/t2v-A14B_1280*720_1_The_dynamic_movement_of_tall,_wispy_grasses_swayin_20251107_124515.mp4"
              autoplay
              muted
              loop
              playsinline
              preload="metadata"
            ></video>
          </div>
          <div class="video-tile" data-video="12">
            <video
              src="assets/VideoGrid/t2v-A14B_1280*720_1_The_vibrant_beauty_of_a_sunflower_field._The_sunfl_20251107_122833.mp4"
              autoplay
              muted
              loop
              playsinline
              preload="metadata"
            ></video>
          </div>
        </div>
      </div>

      <section class="section-overview">
        <h2>Overview</h2>
        <p>
          The LiteAttention framework addresses computational bottlenecks in
          state-of-the-art video generation models. Without requiring any
          fine-tuning of pre-trained models, it leverages the
          <strong>temporal coherence of sparsity patterns</strong>
          across denoising timesteps to significantly speed up the inference
          process.
        </p>
        <p>
          Through a co-design of algorithms and systems, LiteAttention provides
          an end-to-end solution that achieves measurable speedups on real-world
          hardware, offering greater efficiency and lower costs for video
          generation tasks.
        </p>
      </section>

      <section>
        <h2>Key Features</h2>
        <div class="feature-cards">
          <div class="feature-card">
            <h4>Evolutionary Computation Skips</h4>
            <p>
              Identify non-essential tiles once during early denoising and
              propagate skip decisions forward through the entire trajectory.
            </p>
          </div>
          <div class="feature-card">
            <h4>Full-Stage Elimination</h4>
            <p>
              Skip the entire attention iteration (QK product, softmax, PV
              product) for marked tiles, not just partial stages.
            </p>
          </div>
          <div class="feature-card">
            <h4>Error Calibration</h4>
            <p>
              Assign different error bounds to different timesteps, with
              stricter bounds for earlier timesteps that have greater influence.
            </p>
          </div>
          <div class="feature-card">
            <h4>Zero Training Required</h4>
            <p>
              Production-ready, requires no model retraining or architectural
              modifications.
            </p>
          </div>
        </div>
      </section>

      <section class="section-algo">
        <h2>Efficient Sparse Attention via QK-Skip Algorithm</h2>
        <p>
          LiteAttention introduces
          <strong>evolutionary computation skips</strong> that leverage temporal
          coherence in diffusion attention.
        </p>
        <div class="algo-box">
          <h4>QK-Skip Algorithm</h4>
          <p>
            Unlike dynamic methods that repeatedly recompute sparsity at every
            step (incurring 10-20% overhead), LiteAttention maintains a
            Skip-Mask that is updated at each timestep. As the diffusion process
            progresses, the number of tiles marked for skipping gradually
            increases.
          </p>
          <p>
            Once a tile is marked as skippable, the entire attention iteration
            is bypassed for subsequent timesteps, eliminating redundant
            computations without repeated profiling.
          </p>
        </div>
        <p><strong>This approach combines:</strong></p>
        <ul>
          <li>
            <strong>Content adaptivity</strong> of dynamic sparsity (patterns
            derived from actual attention statistics)
          </li>
          <li>
            <strong>Efficiency</strong> of static sparsity (no per-step
            re-evaluation overhead)
          </li>
          <li><strong>Completeness</strong> of full computation elimination</li>
        </ul>
      </section>

      <section>
        <h2>Quantitative Evaluation</h2>
        <p>
          LiteAttention achieves state-of-the-art video quality with significant
          speedups compared to other sparse attention methods, evaluated using
          VBench metrics on production video diffusion models.
        </p>
        <div id="video-comp" class="video-comp-grid">
          <div class="video-comp-item">
            <div class="video-comp-title video-comp-title-strong">
              Baseline - FlashAttention3
            </div>
            <video
              class="sync-video"
              src="assets/VideoComp/FaComp.mp4"
              muted
              autoplay
              playsinline
              loop
              preload="metadata"
            ></video>
          </div>
          <div class="video-comp-item">
            <div class="video-comp-title video-comp-title-lite">
              LiteAttention
            </div>
            <video
              class="sync-video highlight-lite"
              src="assets/VideoComp/LaComp.mp4"
              muted
              autoplay
              playsinline
              loop
              preload="metadata"
            ></video>
          </div>
          <div class="video-comp-item">
            <div class="video-comp-title video-comp-title-muted">
              RadialAttention
            </div>
            <video
              class="sync-video"
              src="assets/VideoComp/RadComp.mp4"
              muted
              autoplay
              playsinline
              loop
              preload="metadata"
            ></video>
          </div>
          <div class="video-comp-item">
            <div class="video-comp-title video-comp-title-muted">
              SparseVideoGen
            </div>
            <video
              class="sync-video"
              src="assets/VideoComp/SvgComp.mp4"
              muted
              autoplay
              playsinline
              loop
              preload="metadata"
            ></video>
          </div>
        </div>

        <h3>Wan2.2-14B Detailed Comparison</h3>
        <table>
          <thead>
            <tr>
              <th>Method</th>
              <th>AQ ↑</th>
              <th>BC ↑</th>
              <th>DD ↑</th>
              <th>IQ ↑</th>
              <th>SC ↑</th>
              <th>TF ↑</th>
              <th>TS ↑</th>
              <th>Sparsity ↑</th>
              <th>Runtime ↓</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>FlashAttention3</td>
              <td>0.693</td>
              <td>0.977</td>
              <td><strong>0.583</strong></td>
              <td><strong>72.73</strong></td>
              <td>0.970</td>
              <td>0.953</td>
              <td>0.133</td>
              <td>0%</td>
              <td>1473 sec</td>
            </tr>
            <tr>
              <td>SparseVideoGen</td>
              <td><em>0.689</em></td>
              <td>0.962</td>
              <td><em>0.417</em></td>
              <td><em>72.24</em></td>
              <td>0.961</td>
              <td><em>0.952</em></td>
              <td><em>0.061</em></td>
              <td><strong>66%</strong></td>
              <td><em>1022 sec</em></td>
            </tr>
            <tr>
              <td>RadialAttention</td>
              <td>0.682</td>
              <td><em>0.974</em></td>
              <td><strong>0.500</strong></td>
              <td><strong>72.73</strong></td>
              <td><em>0.967</em></td>
              <td>0.947</td>
              <td><em>0.061</em></td>
              <td><strong>66%</strong></td>
              <td>1207 sec</td>
            </tr>
            <tr style="background: #fffef0">
              <td><strong>LiteAttention</strong></td>
              <td><strong>0.698</strong></td>
              <td><strong>0.977</strong></td>
              <td><strong>0.500</strong></td>
              <td>71.44</td>
              <td><strong>0.969</strong></td>
              <td><strong>0.953</strong></td>
              <td><strong>0.135</strong></td>
              <td><em>32%</em></td>
              <td><strong>893 sec</strong></td>
            </tr>
          </tbody>
        </table>
        <p
          style="
            font-size: 0.85em;
            color: #666;
            margin-top: 15px;
            text-align: center;
          "
        >
          <em
            >Best results in <strong>bold</strong>, second-best in
            <em>italic</em></em
          ><br />
          <em
            ><strong>VBench Metrics:</strong> AQ (Aesthetic Quality), BC
            (Background Consistency), DD (Dynamic Degree), IQ (Imaging Quality),
            SC (Subject Consistency), TF (Temporal Flickering), TS (Temporal
            Style)</em
          >
        </p>

        <h3>Speedup Analysis</h3>
        <p style="margin-top: 30px">
          LiteAttention achieves significant speedups over FlashAttention3
          baseline:
        </p>
        <ul>
          <li style="margin: 10px 0">
            <strong>Wan2.1-14B</strong>: 1707 sec → 902 sec =
            <strong>1.89× speedup</strong> (47% time reduction)
          </li>
          <li style="margin: 10px 0">
            <strong>Wan2.2-14B</strong>: 1473 sec → 893 sec =
            <strong>1.65× speedup</strong> (39% time reduction)
          </li>
        </ul>
        <p>
          LiteAttention achieves the <strong>best runtime</strong> on both
          models while maintaining <strong>superior quality metrics</strong>
          compared to SparseVideoGen and RadialAttention.
        </p>

        <h3>Ablation Study: Sparsity vs Runtime</h3>
        <p style="margin-top: 30px">
          Our ablation studies demonstrate that runtime improvement scales with
          attention sparsity:
        </p>
        <table>
          <thead>
            <tr>
              <th>Sparsity</th>
              <th>Self-Attention Runtime</th>
              <th>Runtime Improvement</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>0%</td>
              <td>695 sec</td>
              <td>0% (baseline)</td>
            </tr>
            <tr>
              <td>21%</td>
              <td>573 sec</td>
              <td>18%</td>
            </tr>
            <tr>
              <td>42%</td>
              <td>418 sec</td>
              <td>40%</td>
            </tr>
            <tr>
              <td>57%</td>
              <td>308 sec</td>
              <td>56%</td>
            </tr>
            <tr>
              <td>77%</td>
              <td>163 sec</td>
              <td>77%</td>
            </tr>
          </tbody>
        </table>
        <p style="margin-top: 20px">
          The near-linear scaling between sparsity and runtime improvement
          demonstrates the efficiency of our QK-Skip algorithm.
        </p>
      </section>

      <section class="section-blue">
        <h2>Gallery - Wan2.1-14B Generation Times</h2>
        <p>
          LiteAttention provides significant speedups on video generation tasks.
          Below are generation times and visual comparisons at different
          threshold settings:
        </p>
        <div class="video-grid">
          <div class="video-item">
            <img src="./assets/wan_outputs/baseline.gif" alt="Baseline" />
            <div class="video-caption">Baseline (no skip)</div>
            <div class="video-time">23m 51s</div>
          </div>
          <div class="video-item">
            <img src="./assets/wan_outputs/minus10.gif" alt="Threshold -10" />
            <div class="video-caption">Threshold -10</div>
            <div class="video-time">14m 19s</div>
          </div>
          <div class="video-item">
            <img src="./assets/wan_outputs/minus3.gif" alt="Threshold -3" />
            <div class="video-caption">Threshold -3</div>
            <div class="video-time">11m 46s</div>
          </div>
          <div class="video-item">
            <img src="./assets/wan_outputs/zero.gif" alt="Threshold 0" />
            <div class="video-caption">Threshold 0</div>
            <div class="video-time">8m 31s</div>
          </div>
        </div>
      </section>

      <section>
        <h2>Quick Start</h2>
        <h3>Installation</h3>
        <pre><code>git clone https://github.com/moonmath-ai/LiteAttention.git
cd LiteAttention/hopper
python setup.py install</code></pre>

        <h3>Basic Usage</h3>
        <pre><code>from lite_attention import LiteAttention

# Initialize with threshold
attn = LiteAttention(threshold=-6.0)

# Use in your model
output = attn(query, key, value, scale)</code></pre>

        <p style="margin-top: 25px">
          See the
          <a
            href="https://github.com/moonmath-ai/LiteAttention"
            style="color: #0066cc; font-weight: 600"
            >GitHub repository</a
          >
          for detailed documentation and examples.
        </p>
      </section>

      <section class="section-blue">
        <h2>BibTeX</h2>
        <pre><code>@misc{shmilovich2025liteattentiontemporalsparseattention,
                title={LiteAttention: A Temporal Sparse Attention for Diffusion Transformers}, 
                author={Dor Shmilovich and Tony Wu and Aviad Dahan and Yuval Domb},
                year={2025},
                eprint={2511.11062},
                archivePrefix={arXiv},
                primaryClass={cs.CV},
                url={https://arxiv.org/abs/2511.11062}, 
          }
}</code></pre>
      </section>

      <section>
        <h2>Acknowledgements</h2>
        <p>
          LiteAttention is built on top of
          <a
            href="https://github.com/Dao-AILab/flash-attention"
            style="color: #0066cc"
            >FlashAttention3</a
          >
          by Tri Dao and contributors. We thank the FlashAttention team for
          their foundational work on efficient attention mechanisms.
        </p>
        <p>
          We also thank the teams behind
          <a
            href="https://github.com/svg-project/Sparse-VideoGen"
            style="color: #0066cc"
            >SparseVideoGen</a
          >,
          <a
            href="https://github.com/mit-han-lab/radial-attention"
            style="color: #0066cc"
            >RadialAttention</a
          >,
          <a
            href="https://github.com/thu-ml/SageAttention"
            style="color: #0066cc"
            >SageAttention</a
          >,
          <a href="https://github.com/Wan-Video/Wan2.1" style="color: #0066cc"
            >Wan2.1</a
          >, and
          <a
            href="https://github.com/Lightricks/LTX-Video"
            style="color: #0066cc"
            >LTX-Video</a
          >
          for their insights and benchmarking support.
        </p>
      </section>
    </div>

    <footer>
      <p>
        &copy; 2025 MoonMath.ai. Released under BSD-3-Clause and MIT licenses.
      </p>
      <p style="margin-top: 10px">
        <a href="https://github.com/moonmath-ai/LiteAttention">GitHub</a> |
        <a href="https://moonmath.ai">MoonMath.ai</a>
      </p>
    </footer>
    <script>
      (function syncComparisonVideos() {
        const container = document.getElementById("video-comp");
        if (!container) return;
        const vids = Array.from(container.querySelectorAll("video.sync-video"));
        if (vids.length !== 4) return;

        let readyCount = 0;
        let started = false;

        function tryStartAll() {
          if (readyCount === vids.length && !started) {
            started = true;
            vids.forEach((v) => {
              try {
                v.currentTime = 0;
                const p = v.play();
                if (p && typeof p.then === "function") {
                  p.catch(() => {});
                }
              } catch (_) {}
            });
          }
        }

        vids.forEach((v) => {
          v.addEventListener(
            "canplaythrough",
            () => {
              readyCount += 1;
              tryStartAll();
            },
            { once: true }
          );
          // Keep loops aligned by resetting all when one loops
          v.addEventListener("ended", () => {
            vids.forEach((x) => {
              x.currentTime = 0;
              x.play().catch(() => {});
            });
          });
        });
      })();
    </script>
  </body>
</html>
